# -*- coding: utf-8 -*-
"""baseline subset vs fine-tuned test

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mK0UIpnj7NNZU7Mn9OLz0TErLXrAx1Xk

Install dependencies
"""

!pip -q install --force-reinstall --no-cache-dir \
  numpy==2.0.2 \
  pandas==2.2.2 \
  scipy==1.14.1 \
  scikit-learn==1.6.1

"""check version"""

import numpy as np, pandas as pd, scipy, sklearn
print("numpy:", np.__version__)
print("pandas:", pd.__version__)
print("scipy:", scipy.__version__)
print("sklearn:", sklearn.__version__)

"""Imports + seed"""

import os, json, random, gc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import files
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
)

import torch

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

"""Upload CSV + load DataFrame"""

uploaded = files.upload()
CSV_PATH = list(uploaded.keys())[0]

df = pd.read_csv(CSV_PATH)
print("Loaded:", CSV_PATH)
print("Shape:", df.shape)
print("Columns:", list(df.columns))
df.head()

"""Sanity checks + label mapping"""

TEXT_COL = "data"
LABEL_COL = "labels"

assert TEXT_COL in df.columns, f"Missing text column '{TEXT_COL}'"
assert LABEL_COL in df.columns, f"Missing label column '{LABEL_COL}'"

df = df.dropna(subset=[TEXT_COL, LABEL_COL]).reset_index(drop=True)

label_names = sorted(df[LABEL_COL].unique().tolist())
label2id = {l:i for i,l in enumerate(label_names)}
id2label = {i:l for l,i in label2id.items()}

print("Num labels:", len(label_names))
print("Labels:", label_names)
print(df[LABEL_COL].value_counts())

"""Stratified train/val/test split (80/10/10)"""

train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    stratify=df[LABEL_COL],
    random_state=SEED
)

train_df, val_df = train_test_split(
    train_df,
    test_size=0.125,  # 0.125 of 0.8 = 0.10 of total => 80/10/10
    stratify=train_df[LABEL_COL],
    random_state=SEED
)

train_df = train_df.reset_index(drop=True)
val_df   = val_df.reset_index(drop=True)
test_df  = test_df.reset_index(drop=True)

print("train:", train_df.shape, "val:", val_df.shape, "test:", test_df.shape)
print("train label dist:\n", train_df[LABEL_COL].value_counts(normalize=True))
print("val label dist:\n", val_df[LABEL_COL].value_counts(normalize=True))
print("test label dist:\n", test_df[LABEL_COL].value_counts(normalize=True))

"""Build HF Datasets + add label_id"""

train_df["label_id"] = train_df[LABEL_COL].map(label2id)
val_df["label_id"]   = val_df[LABEL_COL].map(label2id)
test_df["label_id"]  = test_df[LABEL_COL].map(label2id)

ds_train = Dataset.from_pandas(train_df)
ds_val   = Dataset.from_pandas(val_df)
ds_test  = Dataset.from_pandas(test_df)

print(ds_train, ds_val, ds_test)

"""# Part A — Prompt Baseline (FLAN-T5)

Load FLAN-T5 (baseline prompt classifier)
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

BASELINE_MODEL_ID = "google/flan-t5-base"

baseline_tokenizer = AutoTokenizer.from_pretrained(BASELINE_MODEL_ID)
baseline_model = AutoModelForSeq2SeqLM.from_pretrained(BASELINE_MODEL_ID)

LABEL_SET = ", ".join(label_names)

def build_prompt(text: str) -> str:
    return (
        "You are a text classification assistant.\n"
        f"Task: Classify the article into exactly one of these labels: {LABEL_SET}.\n"
        "Return only the label name and nothing else.\n\n"
        f"Article:\n{text}\n\n"
        "Label:"
    )

def normalize_prediction(pred: str) -> str:
    pred_clean = pred.strip().replace('"', '').replace("'", "")
    pred_lower = pred_clean.lower()

    for lab in label_names:
        if pred_lower == lab.lower():
            return lab
    for lab in label_names:
        if lab.lower() in pred_lower:
            return lab

    return "UNKNOWN"

@torch.no_grad()
def classify_with_flan_t5(text: str, max_new_tokens: int = 6) -> str:
    prompt = build_prompt(text)
    inputs = baseline_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    outputs = baseline_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        num_beams=1
    )
    decoded = baseline_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return normalize_prediction(decoded)

"""Evaluate baseline on a stratified subset (fast)"""

N = 300
df_test = ds_test.to_pandas()

df_small, _ = train_test_split(
    df_test,
    train_size=min(N, len(df_test)),
    stratify=df_test[LABEL_COL],
    random_state=SEED
)

test_small = ds_test.select(df_small.index.tolist())

y_true_b = []
y_pred_b = []

for ex in test_small:
    y_true_b.append(ex[LABEL_COL])
    y_pred_b.append(classify_with_flan_t5(ex[TEXT_COL]))

print(classification_report(y_true_b, y_pred_b, labels=label_names, digits=3, zero_division=0))

cm = confusion_matrix(y_true_b, y_pred_b, labels=label_names)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)

plt.figure(figsize=(6,6))
disp.plot(values_format="d")
plt.xticks(rotation=45, ha="right")
plt.title("Baseline (Prompt → FLAN-T5) Confusion Matrix")
plt.show()

"""Save baseline predictions + metrics"""

os.makedirs("artifacts", exist_ok=True)

baseline_report = classification_report(
    y_true_b, y_pred_b, labels=label_names, output_dict=True, zero_division=0
)

with open("artifacts/baseline_metrics.json", "w") as f:
    json.dump(baseline_report, f, indent=2)

pd.DataFrame({"true": y_true_b, "pred": y_pred_b}).to_csv("artifacts/baseline_preds_subset.csv", index=False)

print("Saved: artifacts/baseline_metrics.json, artifacts/baseline_preds_subset.csv")

"""# Part B — LoRA Fine-Tuning (F2LLM-0.6B, seq classification)

Tokenize datasets (for sequence classification)
"""

from transformers import AutoTokenizer

FT_MODEL_ID = "codefuse-ai/F2LLM-0.6B"
tokenizer = AutoTokenizer.from_pretrained(FT_MODEL_ID, use_fast=True)

MAX_LEN = 192

def tok(batch):
    return tokenizer(
        batch[TEXT_COL],
        truncation=True,
        max_length=MAX_LEN,
        padding="max_length",
    )

ds_train_tok = ds_train.map(tok, batched=True)
ds_val_tok   = ds_val.map(tok, batched=True)
ds_test_tok  = ds_test.map(tok, batched=True)

# keep only tensors needed by Trainer
keep_cols = ["input_ids", "attention_mask", "label_id"]
ds_train_tok = ds_train_tok.remove_columns([c for c in ds_train_tok.column_names if c not in keep_cols])
ds_val_tok   = ds_val_tok.remove_columns([c for c in ds_val_tok.column_names if c not in keep_cols])
ds_test_tok  = ds_test_tok.remove_columns([c for c in ds_test_tok.column_names if c not in keep_cols])

# Trainer expects "labels"
ds_train_tok = ds_train_tok.rename_column("label_id", "labels")
ds_val_tok   = ds_val_tok.rename_column("label_id", "labels")
ds_test_tok  = ds_test_tok.rename_column("label_id", "labels")

ds_train_tok.set_format("torch")
ds_val_tok.set_format("torch")
ds_test_tok.set_format("torch")

print(ds_train_tok, ds_val_tok, ds_test_tok)

"""Load seq-class model + pad token + speed settings"""

from transformers import AutoModelForSequenceClassification

# cleanup (helpful if you previously tried other runs)
gc.collect()
torch.cuda.empty_cache()

model = AutoModelForSequenceClassification.from_pretrained(
    FT_MODEL_ID,
    num_labels=len(label_names),
    id2label=id2label,
    label2id=label2id,
    device_map="auto",
)

# Set pad token (required for batch sizes > 1)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False

# memory saver (trade compute for memory)
model.gradient_checkpointing_enable()

print("pad_token:", tokenizer.pad_token, tokenizer.pad_token_id)
print("model pad_token_id:", model.config.pad_token_id)

"""Attach LoRA adapters"""

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj","k_proj","v_proj","o_proj"],
)

model = get_peft_model(model, lora_config)

# ensure pad_token_id survives PEFT wrapping
model.config.pad_token_id = tokenizer.pad_token_id
if hasattr(model, "get_base_model"):
    base = model.get_base_model()
    if hasattr(base, "config"):
        base.config.pad_token_id = tokenizer.pad_token_id

model.print_trainable_parameters()

"""Trainer + bf16 training (fastest on your T4)"""

from transformers import TrainingArguments, Trainer

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision_macro": precision_score(labels, preds, average="macro", zero_division=0),
        "recall_macro": recall_score(labels, preds, average="macro", zero_division=0),
        "f1_macro": f1_score(labels, preds, average="macro", zero_division=0),
        "f1_weighted": f1_score(labels, preds, average="weighted", zero_division=0),
    }

args = TrainingArguments(
    output_dir="outputs_f2llm_lora",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=4,  # effective batch 16
    learning_rate=2e-4,
    num_train_epochs=3,
    weight_decay=0.01,

    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=25,

    bf16=True,
    fp16=False,

    optim="adamw_torch_fused",
    report_to="none",

    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds_train_tok,
    eval_dataset=ds_val_tok,
    compute_metrics=compute_metrics,
)

trainer.train()

"""Evaluate LoRA model on test set (full report + confusion)"""

pred = trainer.predict(ds_test_tok)
y_true = pred.label_ids
y_hat  = np.argmax(pred.predictions, axis=-1)

print(classification_report(
    y_true, y_hat,
    target_names=label_names,
    digits=3,
    zero_division=0
))

cm = confusion_matrix(y_true, y_hat, labels=list(range(len(label_names))))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)

plt.figure(figsize=(6,6))
disp.plot(values_format="d")
plt.xticks(rotation=45, ha="right")
plt.title("F2LLM-0.6B + LoRA — Test Confusion Matrix")
plt.show()

"""Save LoRA adapters + tokenizer + metrics"""

os.makedirs("artifacts", exist_ok=True)

# Save adapters only (lightweight)
model.save_pretrained("artifacts/f2llm_lora_adapters")
tokenizer.save_pretrained("artifacts/f2llm_lora_adapters")

# Save test metrics
slm_report = classification_report(
    y_true, y_hat,
    target_names=label_names,
    output_dict=True,
    zero_division=0
)
with open("artifacts/slm_lora_test_metrics.json", "w") as f:
    json.dump(slm_report, f, indent=2)

pd.DataFrame({"true_id": y_true, "pred_id": y_hat}).to_csv("artifacts/slm_lora_test_preds.csv", index=False)

print("Saved adapters + metrics to artifacts/")

"""Side-by-side comparison table (baseline subset vs fine-tuned test)"""

def extract_summary(report_dict):
    # report_dict is classification_report(output_dict=True)
    return {
        "accuracy": report_dict.get("accuracy", None),
        "macro_f1": report_dict["macro avg"]["f1-score"],
        "weighted_f1": report_dict["weighted avg"]["f1-score"],
    }

baseline_summary = extract_summary(baseline_report)
slm_summary = extract_summary(slm_report)

pd.DataFrame([
    {"model": "Prompt baseline (FLAN-T5) [subset]", **baseline_summary},
    {"model": "F2LLM-0.6B + LoRA [test]", **slm_summary},
])